# Recurrent_Neural_Network_Basics_Without_ML_Library
In this repository I have tried to explain basics of Recurrent Neural Networks and math used behind it. The Recurrent Neural Networks 
are used if your current output depends upon present as well as past outputs/inputs. In RNNs along with the current input your networks
previous hidden state is also taken into consideration, so it has the capability of remembering the past inputs thus understanding 
the context. So the RNNs are widely used in the Language models. In this repository we will consider the addition of 2 numbers with
carry forward. In bit by bit addition your carry is carried for the next output and that output is dependent on the previous inputs 
as well, so we can use RNN to solve this problem, our motto in this discussion is to understand fundamentals behind RNN and math 
used behind it.
