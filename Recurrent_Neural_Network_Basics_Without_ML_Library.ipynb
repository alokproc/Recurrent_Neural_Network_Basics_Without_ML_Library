{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks Without ML Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2018 Google I/O, Google introduced a feature which can be used to book your appointments through a single call. You just have to tell the google assistant that when and where you want to book a reservation/apointment then assistant will call the respected authority, interact with them and will tell you conclusion of the discussion. This feature of assistant is what google calls as <b>Google Duplex</b>. For this feature, assistant has to remember whole context during the call and for that google has used the Recurrent Neural Networks. RNNs are great if your present output depends upon present and past inputs/outputs as well. Now let us discuss basics about RNN,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple and basic RNN is drawn in following figure,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/RNNReprentation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you find any difference between A simple Feedforward network and Recurrent network by observing just above diagram ? Yes! Instead of just forward propagating through weights and input we will consider the previous hidden state as well while calculating the output of the layer. So the arrows of the hidden layers are pointing to the output and itself also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But why we should consider the previous hidden state ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well consider the trained feedforward network, the hidden state of that network represents all the inputs on which it has learned while training and gives the output accordingly. In RNN your current state output does depends upon your current input and past inputs. So if we provide the previous hidden state while calculating the current output then its like considering all previous inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider the 3 layered Rnn (I/p, Hidden, Output) as shown in below figure and discuss cases for forward propagation and backward propagation,\n",
    "Now for forward propagation we have following equations,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/3_layer_RNN.png\" style=\"height:500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output_of_layer1 = $g$( inputs  * weights $_{(W1)}$ + previous hidden state$ * W_{hh})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output_of_layer2 = $g($output_of_layer1 * W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where,\n",
    "g(x) is a activation function <br/>\n",
    "W1 - Weights Input-hidden <br/>\n",
    "$W_{hh}$ - Hidden to hidden matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here Output_of_layer2 is the output of our network. While training we will compare that output with the actual output, calculate error/loss, backpropagate through our network to update weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets write the equation for updating the weights while backpropagating,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W2 = W2 + Gradient of output of Network * Output_of_layer1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient of output of Network is calculated by multiplying output error with the derivation of the activation function at Layer2 output value.<br/>\n",
    "Now for updating the weights of I/p Layer to hidden layer we have to calculate the gradient of the output produced by the layer_1 lets call it as layer_1_gradients. and the equation for calculting the gradient is,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer_1_gradients = future_layer1_gradient * W$_{hh}$ + Gradient of output of Network * W2 * Gradient of output of layer_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The future layer1 gradients are the values of the layer_1_gradients 1 timestep back while backpropagation.<br/>\n",
    "Then we update the weights W1 as,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W1 = W1 + inputs * layer_1_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just have to update the Whh matrix as follows,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W_{hh} = W_{hh}$ + previous_hidden_layer * layer_1_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here previous_hidden_layer is the previous time step hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider the addition of two numbers bit by bit. During addition if both the bits are 1 then addition is 0 and carry is 1, that carry is carried forward for next input to add. So current input is dependent on output of the previous addition. We can use RNN for the problem. We will use the same network as shown is above diagram except in the hidden layer we will use 16 neurons. So that our input layer have 2 input neurons, hidden layer have 16 neurons and output layer has only 1 neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be needing labeled data for the training, for that we will add two numbers and consider the addition as our label/Expected output.\n",
    "Steps for training,<br/>\n",
    "1. Consider two 8 bit binary numbers.\n",
    "2. Take 1 bit from each number from RHS and give input to the network.\n",
    "3. Calculate output through forward propagation\n",
    "4. While calculating the output save the hidden state as it will be used for Further calculations\n",
    "5. Repeat Step 1,2,3,4 for 8 times as we are considering addition for 2 8 bit numbers.\n",
    "6. After 5 Now we have to do backpropagation as 2 8 bit numbers are added by the network.\n",
    "7. For backpropagation calculate necessary values and also save the gradients as it will be used for further calculations.\n",
    "8. Repeat step 7 8 times.\n",
    "9. The steps 1-8 completes 1 iteration, so repeat those steps 10000 times and our network will train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now coding time,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_values = {}\n",
    "no_of_bits_binary = 8\n",
    "max_val = (2**no_of_bits_binary)\n",
    "for i in range(max_val):\n",
    "    binary_values[i] = np.unpackbits(np.array([i],dtype=np.uint8).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function forward propagation\n",
    "def forwardPropagate(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "# sigmoid derivative backward propagation\n",
    "def backPropagate(x):\n",
    "    return x*(1-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:[3.53572324]\n",
      "Predicted [0 0 0 0 0 0 0 0]\n",
      "Expected: [0 1 1 0 0 1 0 0]\n",
      "1 + 99 = 0\n",
      "Error:[1.22344201]\n",
      "Predicted [0 0 1 0 0 0 1 1]\n",
      "Expected: [0 0 1 0 0 0 1 1]\n",
      "16 + 19 = 35\n",
      "Error:[3.25544081]\n",
      "Predicted [0 1 1 1 1 1 1 0]\n",
      "Expected: [1 1 1 0 1 0 1 0]\n",
      "110 + 124 = 126\n",
      "Error:[1.82797663]\n",
      "Predicted [0 1 1 1 0 0 1 1]\n",
      "Expected: [0 1 1 1 0 1 1 1]\n",
      "11 + 108 = 115\n",
      "Error:[0.85373709]\n",
      "Predicted [1 0 1 0 1 1 1 1]\n",
      "Expected: [1 0 1 0 1 1 1 1]\n",
      "91 + 84 = 175\n",
      "Error:[1.36818582]\n",
      "Predicted [0 1 0 1 1 0 1 0]\n",
      "Expected: [0 1 0 1 1 0 1 0]\n",
      "61 + 29 = 90\n",
      "Error:[1.93960733]\n",
      "Predicted [0 1 0 1 1 1 1 0]\n",
      "Expected: [0 1 0 1 1 0 0 0]\n",
      "73 + 15 = 94\n",
      "Error:[0.22906529]\n",
      "Predicted [1 1 0 1 0 0 0 1]\n",
      "Expected: [1 1 0 1 0 0 0 1]\n",
      "112 + 97 = 209\n",
      "Error:[0.28318212]\n",
      "Predicted [0 0 1 1 0 1 1 1]\n",
      "Expected: [0 0 1 1 0 1 1 1]\n",
      "11 + 44 = 55\n",
      "Error:[0.16649797]\n",
      "Predicted [0 1 0 1 0 1 0 1]\n",
      "Expected: [0 1 0 1 0 1 0 1]\n",
      "51 + 34 = 85\n"
     ]
    }
   ],
   "source": [
    "#hyperparameters\n",
    "inputLayerSize = 2\n",
    "hiddenLayerSize = 16\n",
    "outputLayerSize = 1\n",
    "\n",
    "W1 = 2 * np.random.random((inputLayerSize,hiddenLayerSize)) - 1\n",
    "W2 =  2 * np.random.random((hiddenLayerSize,outputLayerSize)) - 1\n",
    "Whh =  2 * np.random.random((hiddenLayerSize,hiddenLayerSize)) - 1\n",
    "\n",
    "#For updating these values we will require temporary variables with same size\n",
    "W1_update = np.zeros_like(W1)\n",
    "W2_update = np.zeros_like(W2)\n",
    "Whh_update = np.zeros_like(Whh)\n",
    "\n",
    "#We need labeled data for labels we will simply add two numbers and store its output as the label\n",
    "#10000 is the number of iterations\n",
    "for i in range(10000):\n",
    "    a_int = np.random.randint(max_val/2)\n",
    "    b_int = np.random.randint(max_val/2)\n",
    "    a = binary_values[a_int]\n",
    "    b = binary_values[b_int]\n",
    "    c = binary_values[a_int + b_int]\n",
    "    \n",
    "    d = np.zeros_like(c)\n",
    "    overall_error = 0\n",
    "    \n",
    "    output_layer_gradients = list()\n",
    "    hidden_layer_gradients = list()\n",
    "    hidden_layer_values = list()\n",
    "    hidden_layer_values.append(np.zeros(hiddenLayerSize))\n",
    "    \n",
    "    #Forward Propagation throug our rnn for each binary digit\n",
    "    for position in range(no_of_bits_binary):\n",
    "        #Input X will take 1 bit from a and 1 bit from b\n",
    "        X = np.array([[a[no_of_bits_binary - position - 1],b[no_of_bits_binary - position - 1]]])\n",
    "        #Output label \n",
    "        y = np.array([[c[no_of_bits_binary - position - 1]]]).T\n",
    "        \n",
    "        #Forward Propagation through layer 1\n",
    "        layer_1 = forwardPropagate(np.dot(X,W1)+np.dot(hidden_layer_values[-1],Whh))\n",
    "        #layer_2 is the predicted output of the network\n",
    "        layer_2 = forwardPropagate(np.dot(layer_1,W2))\n",
    "        #calculate error by subtracting actual value - predicted value\n",
    "        output_error = y - layer_2\n",
    "        \n",
    "        #calculate gradient after each forward propagation\n",
    "        output_layer_gradients.append((output_error)*backPropagate(layer_2))\n",
    "        \n",
    "        overall_error += np.abs(output_error[0])\n",
    "        \n",
    "        #rondoff the values\n",
    "        d[no_of_bits_binary - position - 1] = np.round(layer_2[0][0])\n",
    "        \n",
    "        #Saving the hidden State values\n",
    "        hidden_layer_values.append(copy.deepcopy(layer_1))\n",
    "    future_layer1_gradient = np.zeros(hiddenLayerSize)\n",
    "    \n",
    "    #Now backpropagation\n",
    "    for position in range(no_of_bits_binary):\n",
    "        X = np.array(a[position],b[position])\n",
    "        layer_1 = hidden_layer_values[-position-1]\n",
    "        prev_hidden_layer = hidden_layer_values[-position-2]\n",
    "        output_layer_gradient = output_layer_gradients[-position-1]\n",
    "        layer_1_gradients = (future_layer1_gradient.dot(Whh.T) + output_layer_gradient.dot(W2.T)) * backPropagate(layer_1)\n",
    "        \n",
    "        #Updating all the values\n",
    "        W2_update += np.atleast_2d(layer_1).T.dot(output_layer_gradient)\n",
    "        Whh_update += np.atleast_2d(prev_hidden_layer).T.dot(layer_1_gradients)\n",
    "        W1_update += X.T.dot(layer_1_gradients)\n",
    "        \n",
    "        future_layer1_gradient = layer_1_gradients\n",
    "    #Update all the weights\n",
    "    W1 += W1_update\n",
    "    W2 += W2_update\n",
    "    Whh += Whh_update\n",
    "    \n",
    "    #Clear the updated values\n",
    "    W1_update *= 0\n",
    "    W2_update *= 0\n",
    "    Whh_update *= 0\n",
    "    if (i % 1000 == 0):\n",
    "        out=0\n",
    "        print(\"Error:\" + str(overall_error))\n",
    "        print(\"Predicted \"+str(d))\n",
    "        print(\"Expected: \"+str(c))\n",
    "        for index,bit in enumerate(reversed(d)):\n",
    "             out += bit * pow(2, index)\n",
    "        print(str(a_int)+ \" + \"+str(b_int) + \" = \"+str(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the error gets minimized as training iteration proceeds further. If we give two numbers to our network then we can get result as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddUsingRNN(first_number , second_number):\n",
    "    a = binary_values[first_number]\n",
    "    b = binary_values[second_number]\n",
    "    additionBinary = np.zeros_like(b)\n",
    "    out = 0\n",
    "    hidden_layer = list() \n",
    "    hidden_layer.append(np.zeros(hiddenLayerSize))\n",
    "    for position in range(no_of_bits_binary):\n",
    "        X = np.array([a[no_of_bits_binary - position - 1],b[no_of_bits_binary - position - 1]])\n",
    "        first_layer = forwardPropagate(np.dot(X,W1)+np.dot(hidden_layer[-1],Whh))\n",
    "        second_layer = forwardPropagate(np.dot(first_layer,W2))\n",
    "        hidden_layer.append(first_layer)\n",
    "        additionBinary[no_of_bits_binary - position - 1] = np.round(second_layer)\n",
    "    for index, bit in enumerate(reversed(additionBinary)):\n",
    "        out += bit * pow(2, index)\n",
    "    print(str(first_number) + \" + \" + str(second_number) + \" = \" + str(out))\n",
    "    print(\"First Number \"+str(a))\n",
    "    print(\"Second Number\"+str(b))\n",
    "    print(\"Addition     \"+str(additionBinary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 + 28 = 32\n",
      "First Number [0 0 0 0 0 1 0 0]\n",
      "Second Number[0 0 0 1 1 1 0 0]\n",
      "Addition     [0 0 1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "AddUsingRNN(4,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For forward propagation and getting the output from the network we have to pass two numbers bit by bit to our network, after each pass we have to store the hidden state of the network as it will be used to findout the output at next timestep. If you see the output then at first carry is generated by adding 1 and 1 at 3rd position from RHS and it is forwarded till 7th position. We can say that our network has learned to add two numbers with carry forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main use of RNN is in language models as in language models understanding the context is important for better results. I hope that you understood the basics behind the RNN and why google has used RNN in duplex feature of google assistant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
